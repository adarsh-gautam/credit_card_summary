{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959a1cef",
   "metadata": {},
   "source": [
    "# Extracting all transactions data from all the credit card statements till date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46bb57",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aff1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas import ExcelWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0db38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF password\n",
    "password = \"XXXXXXXX\"\n",
    "\n",
    "# regex used for Axis Bank Flipkart Credit Card. Might need little tweak for other cards\n",
    "regex_dict = {\n",
    "                \"Total Payment Due\":          (r\"\\d{1,2}\\,\\d{3}.\\d{2}\\s*[DC]r|\\d+\\.\\d{2}\\s*[DC]r?\", 0),\n",
    "                \"Minimum Payment Due\":        (r\"\\d{1,2}\\,\\d{3}.\\d{2}\\s*[DC]r|\\d+\\.\\d{2}\\s*[DC]r?\", 1),\n",
    "                \"Statement Period\":           (r\"\\d{2}\\/\\d{2}\\/\\d{4} - \\d{2}\\/\\d{2}\\/\\d{4}\", 0),\n",
    "                \"Payment Due Date\":           (r\"\\d{2}\\/\\d{2}\\/\\d{4}\", 2),\n",
    "                \"Statement Generation Date\":  (r\"\\d{2}\\/\\d{2}\\/\\d{4}\", 3),\n",
    "                \"Credit Card Number\":         (r\"\\d{6}\\*{6}\\d{4}\", 0),\n",
    "                \"Credit Limit\":               (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 2),\n",
    "                \"Available Credit Limit\":     (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 3),\n",
    "                \"Available Cash Limit\":       (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 4),\n",
    "                \"Previous Balance\":           (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 5),\n",
    "                \"Payments\":                   (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 6),\n",
    "                \"Credits\":                    (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 7),\n",
    "                \"Purchase\":                   (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 8),\n",
    "                \"Cash Advance\":               (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 9),\n",
    "                \"Other Debit&Charges\":        (r\"\\d{1,2}\\,\\d{3}.\\d{2}|\\d+\\.\\d{2}?\", 10),\n",
    "                \"=Total Payment Due\":         (r\"\\d{1,2}\\,\\d{3}.\\d{2}\\s*[DC]r|\\d+\\.\\d{2}\\s*[DC]r?\", 0),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be5528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table(content):\n",
    "    \n",
    "    # New Data Frame\n",
    "    df_tab = pd.DataFrame()\n",
    "\n",
    "    # Dates for the transactions\n",
    "    regex_stat_per = regex_dict['Payment Due Date'][0]\n",
    "    matches = re.findall(regex_stat_per, content, re.MULTILINE)\n",
    "    df_tab['DATE'] = matches[4:]\n",
    "    df_tab.head()\n",
    "\n",
    "    # Amount and Cashback \n",
    "    regex_stat_per = regex_dict['Total Payment Due'][0]\n",
    "    matches = re.findall(regex_stat_per, content, re.MULTILINE)\n",
    "    matches = matches[-(df_tab.shape[0]*2):]\n",
    "\n",
    "    AMOUNT_list = []\n",
    "    CASHBACK_list = []\n",
    "\n",
    "    for i in range(0, (df_tab.shape[0]*2)):\n",
    "        if i%2 == 0:\n",
    "            AMOUNT_list.append(matches[i])\n",
    "        else:\n",
    "            CASHBACK_list.append(matches[i])\n",
    "\n",
    "    df_tab['AMOUNT'] = AMOUNT_list\n",
    "    df_tab['CASHBACK EARNED'] = CASHBACK_list\n",
    "    df_tab.head()\n",
    "\n",
    "    # Text Part in Table\n",
    "    regex_stat_per = r'\\n\\d{2}\\/\\d{2}\\/\\d{4}\\s(\\w.*?\\s\\s*)?[\\d]+'\n",
    "    matches = re.findall(regex_stat_per, content, re.MULTILINE)\n",
    "\n",
    "    df_tab['TRANSACTION DETAILS'] = matches\n",
    "    \n",
    "    return df_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e627d41",
   "metadata": {},
   "source": [
    "## Header Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e3e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_credit_header = []\n",
    "\n",
    "# iterating all the files with pdf extension in current folder(\".\")\n",
    "for path in Path(\".\").glob(\"*.pdf\"):\n",
    "    \n",
    "    pdf = open(path, 'rb')\n",
    "    pdf = PyPDF2.PdfFileReader(pdf)\n",
    "    pdf.decrypt(password=password)\n",
    "\n",
    "    # Adding all pages content\n",
    "    content = ''\n",
    "    for i in range(0, pdf.getNumPages()): \n",
    "        content += pdf.getPage(i).extractText()\n",
    "        content += '\\n'\n",
    "\n",
    "    content = content[:-1]\n",
    "    \n",
    "    # getting value for each field from content using regex\n",
    "    header_list = []\n",
    "    for key, val in regex_dict.items():\n",
    "        matches = re.findall(val[0], content, re.MULTILINE)\n",
    "        header_list.append(matches[val[1]])\n",
    "        # print(key, \"-------------------------->\", matches[val[1]])\n",
    "\n",
    "    all_credit_header.append(header_list)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data = all_credit_header, columns = regex_dict.keys())\n",
    "df.loc[:, 'Statement Generation Date'] = pd.to_datetime(df['Statement Generation Date'], format='%d/%m/%Y')\n",
    "df.sort_values('Statement Generation Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50562cfd",
   "metadata": {},
   "source": [
    "## Table Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1060afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# iterating all the files with pdf extension in current folder(\".\")\n",
    "for path in Path(\".\").glob(\"*.pdf\"):\n",
    "    \n",
    "    pdf = open(path, 'rb')\n",
    "    pdf = PyPDF2.PdfFileReader(pdf)\n",
    "    pdf.decrypt(password=password)\n",
    "\n",
    "    # Adding all pages content\n",
    "    content = ''\n",
    "    for i in range(0, pdf.getNumPages()): \n",
    "        content += pdf.getPage(i).extractText()\n",
    "        content += '\\n'\n",
    "\n",
    "    content = content[:-1]\n",
    "    \n",
    "    df_one = extract_table(content)\n",
    "    df_one['File'] = path\n",
    "    df_list.append(df_one)\n",
    "    \n",
    "df_final = pd.concat(df_list)\n",
    "df_final.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a9a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ExcelWriter('All Transactions.xlsx')\n",
    "\n",
    "df_final.to_excel(writer, index=False, sheet_name='Detailed')\n",
    "df.to_excel(writer, index=False, sheet_name='Header')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c8a47",
   "metadata": {},
   "source": [
    "# Note: \n",
    "-     This code is very specific to Axis FLipkart Credit Card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624beb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
